# -*- coding: utf-8 -*-
"""MM-FLOOD_Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swMkxr12Oy0y3j1y4lWqn_EDvo4ASv2G
"""

import rasterio
import numpy as np
import os
from Unet_models import UNet
from focal_loss import BinaryFocalLoss
import matplotlib.pyplot as plt


def cccut(img, min_percent = 2, max_percent = 98):
    lo, hi = np.percentile(img, (min_percent, max_percent))
    # Apply linear "stretch" - lo goes to 0, and hi goes to 1
    res_img = (img.astype(np.float32) - lo) / (hi - lo)
    # Multiply by 1, clamp range to [0, 1] and convert to np.float32
    res_img = np.maximum(np.minimum(res_img * 1, 1), 0).astype(np.float32)
    return res_img

path = "C:/Users/USER/PycharmProjects/Test_tf_gpu/MMflood_train/PLOTO/rtc_gamma0_budapest/"

# Open the VV band GeoTIFF file
with rasterio.open(path+'VV/budapest_3.tif') as vv_src:
    vv_band = vv_src.read(1)  # Read VV band data

# Open the VH band GeoTIFF file
with rasterio.open(path+'VH/budapest_3.tif') as vh_src:
    vh_band = vh_src.read(1)  # Read VH band data

# path = "C:/Users/USER/PycharmProjects/Test_tf_gpu/MMflood_train/PLOTO/data/"

# # Open the VV band GeoTIFF file
# with rasterio.open(path+'2024-01-14-00_00_2024-01-14-23_59_Sentinel-1_AWS-IW-VVVH_VH_(Raw).tiff') as vv_src:
#     vv_band = vv_src.read(1)  # Read VV band data

# # Open the VH band GeoTIFF file
# with rasterio.open(path+'2024-01-14-00_00_2024-01-14-23_59_Sentinel-1_AWS-IW-VVVH_VV_(Raw).tiff') as vh_src:
#     vh_band = vh_src.read(1)  # Read VH band data

# Stack the bands together to create a 2-channel image
stacked_image = cccut(np.stack((vv_band, vh_band), axis=-1))
print(stacked_image.shape)

print(stacked_image.shape, stacked_image.min(), stacked_image.max(), stacked_image.dtype, stacked_image.mean(), stacked_image.std())

# Separate the bands
vv_band = stacked_image[:,:,0]  # VV band
vh_band = stacked_image[:,:,1]  # VH band

# Plot VV band
plt.figure(figsize=(8, 6))
plt.imshow(vv_band, cmap='gray')
plt.title('VV Band')
plt.colorbar()
plt.show()

# Plot VH band
plt.figure(figsize=(8, 6))
plt.imshow(vh_band, cmap='gray')
plt.title('VH Band')
plt.colorbar()
plt.show()

# Define function to split image into patches, handling edges
def split_image_into_patches(image, patch_size):
    patches = []
    height, width, channels = image.shape
    for y in range(0, height, patch_size):
        for x in range(0, width, patch_size):
            patch = image[y:min(y+patch_size, height), x:min(x+patch_size, width)]
            patches.append(patch)
    return np.array(patches)

# Define function to reconstruct image from patches, handling edges
def reconstruct_image_from_patches(patches, original_shape):
    reconstructed_image = np.zeros(original_shape)
    idx = 0
    patch_size = patches.shape[1]
    for y in range(0, original_shape[0], patch_size):
        for x in range(0, original_shape[1], patch_size):
            patch = patches[idx]
            patch_height, patch_width, _ = patch.shape
            reconstructed_image[y:y+patch_height, x:x+patch_width] = patch
            idx += 1
    return reconstructed_image

# Pad the initial image to size 2560x2560
def pad_image(image, target_size):
    height, width, channels = image.shape
    padded_image = np.zeros((target_size, target_size, channels), dtype=image.dtype)
    pad_height = (target_size - height) // 2
    pad_width = (target_size - width) // 2
    padded_image[pad_height:pad_height+height, pad_width:pad_width+width, :] = image
    return padded_image

from tensorflow.keras.models import load_model
# from tensorflow.keras.models import Model

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate
from tensorflow.keras.models import Model

def unet_model(input_shape):
    # Define input layer
    inputs = Input(input_shape)

    # Contracting path
    conv1 = Conv2D(16, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(16, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(32, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(32, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(64, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(64, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(128, 3, activation='relu', padding='same')(pool3)
    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)
    drop4 = Dropout(0.1)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(256, 3, activation='relu', padding='same')(pool4)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)
    drop5 = Dropout(0.1)(conv5)

    # Expansive path
    up6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(drop5)
    up6 = concatenate([up6, drop4], axis=3)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)

    up7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)
    up7 = concatenate([up7, conv3], axis=3)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(up7)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)

    up8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7)
    up8 = concatenate([up8, conv2], axis=3)
    conv8 = Conv2D(32, 3, activation='relu', padding='same')(up8)
    conv8 = Conv2D(32, 3, activation='relu', padding='same')(conv8)

    up9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(conv8)
    up9 = concatenate([up9, conv1], axis=3)
    conv9 = Conv2D(16, 3, activation='relu', padding='same')(up9)
    conv9 = Conv2D(16, 3, activation='relu', padding='same')(conv9)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    # Define model
    model = Model(inputs=inputs, outputs=outputs)
    return model

model_type = 'MM_flood_unet'
results_path = 'C:/Users/USER/PycharmProjects/Test_tf_gpu/MMflood_train/MM_flood_results/'+ model_type+'/'
model_path = results_path+'MMflood_unet'

input_shape = (256,256,2)
unet_model = unet_model(input_shape)

unet_model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])

# unet_model.load_weights(model_path)

# unet_model.save(results_path+'MMflood_unet.h5')

unet_model =  tf.keras.models.load_model(results_path+'MMflood_unet.h5')

# Define patch size
patch_size = 256

# Pad the stacked image
padded_stacked_image = pad_image(stacked_image, patch_size*10)
print(padded_stacked_image.shape)

# Split the image into patches
patches = split_image_into_patches(padded_stacked_image, patch_size)

def find_final_conv_layer(model):
    for layer in reversed(model.layers):
        if isinstance(layer, Conv2D):
            return layer.name
    raise ValueError("No convolutional layer found in the model.")

def generate_grad_cam(model, img_array):
    layer_name = find_final_conv_layer(model)
    grad_model = Model(
        [model.inputs], [model.get_layer(layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_output, predictions = grad_model(img_array)
        loss = predictions[:, 1]

    grads = tape.gradient(loss, conv_output)[0]
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_output = conv_output[0]
    heatmap = tf.reduce_mean(tf.multiply(conv_output, pooled_grads), axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap /= np.max(heatmap)

    return heatmap

# Run prediction on patches
predicted_patches = []
xai_patches = []

for patch in patches:
    # patch = bandeq(patch)
    print(patch.shape, patch.min(), patch.max(), patch.dtype, patch.mean(axis = (0,1,2)), patch.std(axis=(0,1,2)))

    patch = np.expand_dims(patch, axis = 0)
    print("patch : ", patch.shape)

    pred = unet_model.predict(patch)
    prediction = np.squeeze(pred, axis = 0)
    print("pred : ", prediction.shape, prediction.min(), prediction.max(), prediction.dtype)

    # Apply threshold to the predicted mask
    thresh_pred = (prediction > prediction.mean()).astype(int)
    print("thresh pred : ", thresh_pred.shape, thresh_pred.min(), thresh_pred.max(), thresh_pred.dtype)

    heatmap = generate_grad_cam(unet_model, patch)

    # Create a 1x4 subplot
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))

    # Plot the images on each subplot
    axes[0].imshow(prediction[:,:,:], cmap='gray')
    axes[0].set_title('Pred')
    axes[0].axis('off')

    axes[1].imshow(patch[0,:,:,0], cmap='gray')
    axes[1].set_title('VV')
    axes[1].axis('off')

    axes[2].imshow(patch[0,:,:,1], cmap='gray')
    axes[2].set_title('VH')
    axes[2].axis('off')

    axes[3].imshow(heatmap, cmap='gray')
    axes[3].set_title('Grad - CAM')
    axes[3].axis('off')

    # Adjust layout
    plt.tight_layout()
    plt.show()

    predicted_patches.append(prediction)
    xai_patches.append(heatmap)

# Convert the list of predicted patches to a numpy array
predicted_patches = np.array(predicted_patches)
print(predicted_patches.shape)

# Reconstruct the image from predicted patches
reconstructed_image = reconstruct_image_from_patches(predicted_patches, (2560,2560,1))
print(reconstructed_image.shape)

# Reconstruct the image from predicted patches
reconstructed_original_image = reconstruct_image_from_patches(patches, (2560,2560,2))
print(reconstructed_original_image.shape)

# Convert the list of predicted patches to a numpy array
xai_patches = np.array(xai_patches)
print(xai_patches.shape)

# Reconstruct the image from predicted patches
reconstructed_xai_image = reconstruct_image_from_patches(xai_patches, (2560,2560,1))
print(reconstructed_xai_image.shape)

# Plot the reconstructed image
plt.figure(figsize=(8, 8))
plt.imshow(reconstructed_image, cmap='jet', aspect='auto')  # Assuming grayscale image
plt.axis('off')
plt.title('Reconstructed Image')
plt.colorbar()
plt.show()

# Plot the reconstructed image
plt.imshow(reconstructed_original_image[:,:,0], cmap='gray')  # Assuming grayscale image
plt.axis('off')
plt.title('Reconstructed Image')
plt.show()

# Plot the reconstructed image
plt.imshow(reconstructed_original_image[:,:,1], cmap='gray')  # Assuming grayscale image
plt.axis('off')
plt.title('Reconstructed Image')
plt.show()

# Create a 1x4 subplot
fig, axes = plt.subplots(1, 3, figsize=(16, 4))

# Plot the images on each subplot
axes[0].imshow(reconstructed_image, cmap='gray')
axes[0].set_title('Pred')
axes[0].axis('off')

axes[1].imshow(reconstructed_original_image[:,:,0], cmap='gray')
axes[1].set_title('VV')
axes[1].axis('off')

axes[2].imshow(reconstructed_original_image[:,:,1], cmap='gray')
axes[2].set_title('VH')
axes[2].axis('off')

# Adjust layout
plt.tight_layout()
plt.show()